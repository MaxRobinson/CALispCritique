@phdthesis{Marthi:2006,
 author = {Marthi, Bhaskara Mannar},
 advisor = {Russell, Stuart},
 title = {Concurrent Hierarchical Reinforcement Learning},
 year = {2006},
 note = {AAI3253978},
 publisher = {University of California at Berkeley},
 address = {Berkeley, CA, USA},
}

@inproceedings{Andre:2002,
	author = {Andre, David and Russell, Stuart J.},
	title = {State Abstraction for Programmable Reinforcement Learning Agents},
	booktitle = {Eighteenth National Conference on Artificial Intelligence},
	year = {2002},
	isbn = {0-262-51129-0},
	location = {Edmonton, Alberta, Canada},
	pages = {119--125},
	numpages = {7},
	url = {http://dl.acm.org/citation.cfm?id=777092.777114},
	acmid = {777114},
	publisher = {American Association for Artificial Intelligence},
	address = {Menlo Park, CA, USA},
}

@book{Bertsekas:1996,
	author = {Bertsekas, Dimitri P. and Tsitsiklis, John N.},
	title = {Neuro-Dynamic Programming},
	year = {1996},
	isbn = {1886529108},
	edition = {1st},
	publisher = {Athena Scientific},
}

@book{Puterman:1994,
	author = {Puterman, Martin L.},
	title = {Markov Decision Processes: Discrete Stochastic Dynamic Programming},
	year = {1994},
	isbn = {0471619779},
	edition = {1st},
	publisher = {John Wiley \&amp; Sons, Inc.},
	address = {New York, NY, USA},
}

@Article{Watkins:1992,
	author="Watkins, Christopher J. C. H.
	and Dayan, Peter",
	title="Q-learning",
	journal="Machine Learning",
	year="1992",
	month="May",
	day="01",
	volume="8",
	number="3",
	pages="279--292",
	abstract="Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.",
	issn="1573-0565",
	doi="10.1007/BF00992698",
	url="https://doi.org/10.1007/BF00992698"
}

@book{Sutton:1998,
	author = {Sutton, Richard S. and Barto, Andrew G.},
	title = {Introduction to Reinforcement Learning},
	year = {1998},
	isbn = {0262193981},
	edition = {1st},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
}

@InProceedings{CHRLKeepaway:2017,
	title =	"Concurrent Hierarchical Reinforcement Learning for {RoboCup Keepaway}",
	author =	"Aijun Bai and Stuart Russell and Xiaoping Chen",
	publisher =	"Springer",
	year = 	"2017",
	booktitle =	"RoboCup-2017: Robot World Cup XX",
	series =	"Lecture Notes in Computer Science",
}

@inproceedings{Parr:1998,
	author = {Parr, Ronald and Russell, Stuart},
	title = {Reinforcement Learning with Hierarchies of Machines},
	booktitle = {Proceedings of the 1997 Conference on Advances in Neural Information Processing Systems 10},
	series = {NIPS '97},
	year = {1998},
	isbn = {0-262-10076-2},
	location = {Denver, Colorado, USA},
	pages = {1043--1049},
	numpages = {7},
	url = {http://dl.acm.org/citation.cfm?id=302528.302894},
	acmid = {302894},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
} 

@article{Dietterich:2000,
	author = {Dietterich, Thomas G.},
	title = {Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition},
	journal = {J. Artif. Int. Res.},
	issue_date = {August 2000},
	volume = {13},
	number = {1},
	month = nov,
	year = {2000},
	issn = {1076-9757},
	pages = {227--303},
	numpages = {77},
	url = {http://dl.acm.org/citation.cfm?id=1622262.1622268},
	acmid = {1622268},
	publisher = {AI Access Foundation},
	address = {USA},
}

@article{MacAlpine:2018,
	title = {Overlapping Layered Learning},
	journal = {Artificial Intelligence},
	volume = {254},
	pages = {21--43},
	month = {January},
	year = {2018},
	issn = {0004-3702},
	doi = {https://doi.org/10.1016/j.artint.2017.09.001},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370217301066},
	author = {Patrick MacAlpine and Peter Stone},
	publisher = {Elsevier},
	abstract = {Layered learning is a hierarchical machine learning paradigm that
	enables learning of complex behaviors by incrementally learning a series of 
	sub-behaviors. A key feature of layered learning is that higher layers directly 
	depend on the learned lower layers. In its original formulation, lower layers 
	were frozen prior to learning higher layers. This article considers a major 
	extension to the paradigm that allows learning certain behaviors independently, 
	and then later stitching them together by learning at the "seams" where their 
	influences overlap. The UT Austin Villa 2014 RoboCup 3D simulation team, using 
	such overlapping layered learning, learned a total of 19 layered behaviors for 
	a simulated soccer-playing robot, organized both in series and in parallel. To 
	the best of our knowledge this is more than three times the number of layered 
	behaviors in any prior layered learning system. Furthermore, the complete 
	learning process is repeated on four additional robot body types, showcasing 
	its generality as a paradigm for efficient behavior learning. The resulting 
	team won the RoboCup 2014 championship with an undefeated record, scoring 52 
	goals and conceding none. This article includes a detailed experimental 
	analysis of the team's performance and the overlapping layered learning 
	approach that led to its success.}
	wwwnote = {Official version from <a href="https://authors.elsevier.com/a/1Vtn--c5F-HA">Publisher's Webpage</a><br>
	Accompanying videos at <a href="http://www.cs.utexas.edu/~AustinVilla/sim/3dsimulation/overlappingLayeredLearning.html">http://www.cs.utexas.edu/~AustinVilla/sim/3dsimulation/overlappingLayeredLearning.html</a>}
}


@inproceedings{Kulkarni:2016,
	author = {Kulkarni, Tejas D. and Narasimhan, Karthik R. and Saeedi, Ardavan and Tenenbaum, Joshua B.},
	title = {Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation},
	booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
	series = {NIPS'16},
	year = {2016},
	isbn = {978-1-5108-3881-9},
	location = {Barcelona, Spain},
	pages = {3682--3690},
	numpages = {9},
	url = {http://dl.acm.org/citation.cfm?id=3157382.3157509},
	acmid = {3157509},
	publisher = {Curran Associates Inc.},
	address = {USA},
}

@inproceedings{Makar:2001,
	author = {Makar, Rajbala and Mahadevan, Sridhar and Ghavamzadeh, Mohammad},
	title = {Hierarchical Multi-agent Reinforcement Learning},
	booktitle = {Proceedings of the Fifth International Conference on Autonomous Agents},
	series = {AGENTS '01},
	year = {2001},
	isbn = {1-58113-326-X},
	location = {Montreal, Quebec, Canada},
	pages = {246--253},
	numpages = {8},
	url = {http://doi.acm.org/10.1145/375735.376302},
	doi = {10.1145/375735.376302},
	acmid = {376302},
	publisher = {ACM},
	address = {New York, NY, USA},
}

@inproceedings{Russell:2003,
	author = {Russell, Stuart and Zimdars, Andrew L.},
	title = {Q-decomposition for Reinforcement Learning Agents},
	booktitle = {Proceedings of the Twentieth International Conference on International Conference on Machine Learning},
	series = {ICML'03},
	year = {2003},
	isbn = {1-57735-189-4},
	location = {Washington, DC, USA},
	pages = {656--663},
	numpages = {8},
	url = {http://dl.acm.org/citation.cfm?id=3041838.3041921},
	acmid = {3041921},
	publisher = {AAAI Press},
}

@Article{Singh:2000,
	author="Singh, Satinder
	and Jaakkola, Tommi
	and Littman, Michael L.
	and Szepesv{\'a}ri, Csaba",
	title="Convergence Results for Single-Step On-Policy Reinforcement-Learning Algorithms",
	journal="Machine Learning",
	year="2000",
	month="Mar",
	day="01",
	volume="38",
	number="3",
	pages="287--308",
	abstract="An important application of reinforcement learning (RL) is to finite-state control problems and one of the most difficult problems in learning for control is balancing the exploration/exploitation tradeoff. Existing theoretical results for RL give very little guidance on reasonable ways to perform exploration. In this paper, we examine the convergence of single-step on-policy RL algorithms for control. On-policy algorithms cannot separate exploration from learning and therefore must confront the exploration problem directly. We prove convergence results for several related on-policy algorithms with both decaying exploration and persistent exploration. We also provide examples of exploration strategies that can be followed during learning that result in convergence to both optimal values and optimal policies.",
	issn="1573-0565",
	doi="10.1023/A:1007678930559",
	url="https://doi.org/10.1023/A:1007678930559"
}

@article{Todorov:2005,
	author = {Emanuel Todorov and Weiwei Li and Xiuchuan Pan},
	title = {From task parameters to motor synergies: A hierarchical framework for approximately optimal control of redundant manipulators},
	journal = {Journal of Robotic Systems},
	volume = {22},
	number = {11},
	pages = {691-710},
	year = 2005,
	doi = {10.1002/rob.20093},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.20093},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/rob.20093},
	abstract = {Abstract We present a hierarchical framework for approximately optimal control of redundant manipulators. The plant is augmented with a low‐level feedback controller, designed to yield input‐output behavior that captures the task‐relevant aspects of plant dynamics but has reduced dimensionality. This makes it possible to reformulate the optimal control problem in terms of the augmented dynamics, and optimize a high‐level feedback controller without running into the curse of dimensionality. The resulting control hierarchy compares favorably to existing methods in robotics. Furthermore, we demonstrate a number of similarities to (nonhierarchical) optimal feedback control. Besides its engineering applications, the new framework addresses a key unresolved problem in the neural control of movement. It has long been hypothesized that coordination involves selective control of task parameters via muscle synergies, but the link between these parameters and the synergies capable of controlling them has remained elusive. Our framework provides this missing link. © 2005 Wiley Periodicals, Inc.}
}

@inproceedings{mehta:2005,
	title={Multi-agent shared hierarchy reinforcement learning},
	author={Mehta, Neville and Tadepalli, Prasad and Fern, A},
	booktitle={ICML Workshop on Richer Representations in Reinforcement Learning},
	year={2005}
}
