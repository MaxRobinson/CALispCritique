@phdthesis{Marthi:2006,
 author = {Marthi, Bhaskara Mannar},
 advisor = {Russell, Stuart},
 title = {Concurrent Hierarchical Reinforcement Learning},
 year = {2006},
 note = {AAI3253978},
 publisher = {University of California at Berkeley},
 address = {Berkeley, CA, USA},
}

@inproceedings{Andre:2002,
	author = {Andre, David and Russell, Stuart J.},
	title = {State Abstraction for Programmable Reinforcement Learning Agents},
	booktitle = {Eighteenth National Conference on Artificial Intelligence},
	year = {2002},
	isbn = {0-262-51129-0},
	location = {Edmonton, Alberta, Canada},
	pages = {119--125},
	numpages = {7},
	url = {http://dl.acm.org/citation.cfm?id=777092.777114},
	acmid = {777114},
	publisher = {American Association for Artificial Intelligence},
	address = {Menlo Park, CA, USA},
}

@book{Bertsekas:1996,
	author = {Bertsekas, Dimitri P. and Tsitsiklis, John N.},
	title = {Neuro-Dynamic Programming},
	year = {1996},
	isbn = {1886529108},
	edition = {1st},
	publisher = {Athena Scientific},
}

@book{Puterman:1994,
	author = {Puterman, Martin L.},
	title = {Markov Decision Processes: Discrete Stochastic Dynamic Programming},
	year = {1994},
	isbn = {0471619779},
	edition = {1st},
	publisher = {John Wiley \&amp; Sons, Inc.},
	address = {New York, NY, USA},
}

@Article{Watkins:1992,
	author="Watkins, Christopher J. C. H.
	and Dayan, Peter",
	title="Q-learning",
	journal="Machine Learning",
	year="1992",
	month="May",
	day="01",
	volume="8",
	number="3",
	pages="279--292",
	abstract="Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.",
	issn="1573-0565",
	doi="10.1007/BF00992698",
	url="https://doi.org/10.1007/BF00992698"
}

@book{Sutton:1998,
	author = {Sutton, Richard S. and Barto, Andrew G.},
	title = {Introduction to Reinforcement Learning},
	year = {1998},
	isbn = {0262193981},
	edition = {1st},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
}

@InProceedings{CHRLKeepaway:2017,
	title =	"Concurrent Hierarchical Reinforcement Learning for {RoboCup Keepaway}",
	author =	"Aijun Bai and Stuart Russell and Xiaoping Chen",
	publisher =	"Springer",
	year = 	"2017",
	booktitle =	"RoboCup-2017: Robot World Cup XX",
	series =	"Lecture Notes in Computer Science",
}

@inproceedings{Parr:1998,
	author = {Parr, Ronald and Russell, Stuart},
	title = {Reinforcement Learning with Hierarchies of Machines},
	booktitle = {Proceedings of the 1997 Conference on Advances in Neural Information Processing Systems 10},
	series = {NIPS '97},
	year = {1998},
	isbn = {0-262-10076-2},
	location = {Denver, Colorado, USA},
	pages = {1043--1049},
	numpages = {7},
	url = {http://dl.acm.org/citation.cfm?id=302528.302894},
	acmid = {302894},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
} 

@article{Dietterich:2000,
	author = {Dietterich, Thomas G.},
	title = {Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition},
	journal = {J. Artif. Int. Res.},
	issue_date = {August 2000},
	volume = {13},
	number = {1},
	month = nov,
	year = {2000},
	issn = {1076-9757},
	pages = {227--303},
	numpages = {77},
	url = {http://dl.acm.org/citation.cfm?id=1622262.1622268},
	acmid = {1622268},
	publisher = {AI Access Foundation},
	address = {USA},
}

@article{MacAlpine:2018,
	title = {Overlapping Layered Learning},
	journal = {Artificial Intelligence},
	volume = {254},
	pages = {21--43},
	month = {January},
	year = {2018},
	issn = {0004-3702},
	doi = {https://doi.org/10.1016/j.artint.2017.09.001},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370217301066},
	author = {Patrick MacAlpine and Peter Stone},
	publisher = {Elsevier},
	abstract = {Layered learning is a hierarchical machine learning paradigm that
	enables learning of complex behaviors by incrementally learning a series of 
	sub-behaviors. A key feature of layered learning is that higher layers directly 
	depend on the learned lower layers. In its original formulation, lower layers 
	were frozen prior to learning higher layers. This article considers a major 
	extension to the paradigm that allows learning certain behaviors independently, 
	and then later stitching them together by learning at the "seams" where their 
	influences overlap. The UT Austin Villa 2014 RoboCup 3D simulation team, using 
	such overlapping layered learning, learned a total of 19 layered behaviors for 
	a simulated soccer-playing robot, organized both in series and in parallel. To 
	the best of our knowledge this is more than three times the number of layered 
	behaviors in any prior layered learning system. Furthermore, the complete 
	learning process is repeated on four additional robot body types, showcasing 
	its generality as a paradigm for efficient behavior learning. The resulting 
	team won the RoboCup 2014 championship with an undefeated record, scoring 52 
	goals and conceding none. This article includes a detailed experimental 
	analysis of the team's performance and the overlapping layered learning 
	approach that led to its success.}
	wwwnote = {Official version from <a href="https://authors.elsevier.com/a/1Vtn--c5F-HA">Publisher's Webpage</a><br>
	Accompanying videos at <a href="http://www.cs.utexas.edu/~AustinVilla/sim/3dsimulation/overlappingLayeredLearning.html">http://www.cs.utexas.edu/~AustinVilla/sim/3dsimulation/overlappingLayeredLearning.html</a>}
}


@inproceedings{Kulkarni:2016,
	author = {Kulkarni, Tejas D. and Narasimhan, Karthik R. and Saeedi, Ardavan and Tenenbaum, Joshua B.},
	title = {Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation},
	booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
	series = {NIPS'16},
	year = {2016},
	isbn = {978-1-5108-3881-9},
	location = {Barcelona, Spain},
	pages = {3682--3690},
	numpages = {9},
	url = {http://dl.acm.org/citation.cfm?id=3157382.3157509},
	acmid = {3157509},
	publisher = {Curran Associates Inc.},
	address = {USA},
}

@inproceedings{Makar:2001,
	author = {Makar, Rajbala and Mahadevan, Sridhar and Ghavamzadeh, Mohammad},
	title = {Hierarchical Multi-agent Reinforcement Learning},
	booktitle = {Proceedings of the Fifth International Conference on Autonomous Agents},
	series = {AGENTS '01},
	year = {2001},
	isbn = {1-58113-326-X},
	location = {Montreal, Quebec, Canada},
	pages = {246--253},
	numpages = {8},
	url = {http://doi.acm.org/10.1145/375735.376302},
	doi = {10.1145/375735.376302},
	acmid = {376302},
	publisher = {ACM},
	address = {New York, NY, USA},
}
