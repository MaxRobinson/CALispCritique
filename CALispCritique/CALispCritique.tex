\documentclass[jair,twoside,11pt,theapa]{article}
\usepackage{jair, theapa, rawfonts, amssymb}

%\jairheading{1}{2018}{}{}{}
%\ShortHeadings{A Summary of Pairwise Saturations in ILP}
%{Robinson}
\firstpageno{1}

\begin{document}

\title{A Critique of the dissertation ``Concurrent Hierarchical Reinforcement Learning"}


\author{\name Max Robinson \email max.robinson@jhu.edu \\
       \addr Johns Hopkins University,\\
       Baltimore, MD 21218 USA
   }

% For research notes, remove the comment character in the line below.
%\researchnote APril3 

\maketitle

%\begin{abstract}
%\end{abstract}

\section{Introduction}
\label{Introduction}
\cite{Marthi:2006}

\section{Related Work}
\label{Related Work}
Hierarchical reinforcement learning (HRL) aims to take advantage of a hierarchical structure of behavior that can be used to improve learning. Early methods included the idea of hierarchical abstract machines (HAM) \cite {Parr:1998}. HAMs build created the idea of partial programs and builds up the idea of constricting the policy search space through the provided knowledge. In addition, HAMs developed the idea of a hierarchically optimal policy. Marthi's thesis builds upon these concepts heavily. 

A seminal work in HRL to date is the MAXQ algorithm \cite{Dietterich:2000}. MAXQ learning is based on the idea of decomposing the overall MDP into smaller MDPs per task and decomposing the value of the highest level MDP into an additive composition of the smaller MDPs. MAXQ also proved a sort of optimality that is possible known as recursive optimality. Different from hierarchically optimal, recursively optimal policies have the benefit the learning task being completely separate from the rest of the learning tasks. 

More recently there has been continued work in HRL. Modifications of layered hierarchical learning in overlapping layered learning \cite{MacAlpine:2018}, which is also based on task decompositions, aims to provide learning between coordinated tasks. This is not dissimilar to coordination between agents that is discussed in Marthi's thesis. 

In addition, the concepts of temporal decomposition or abstraction have been investigated even recently in combination with deep learning \cite{Kulkarni:2016}. The goal is to incorporate a hierarchical structure into the neural network that allows for lower-level learning tasks to take differing amounts of time, while a higher level policy is learned over internal goals. 

The most similar work to Marthi's is one written by Makar et al. (2001) \cite{Makar:2001} and is actually referenced by Marthi. Makar investigated extending the MAXQ algorithm to each agent in the multi-agent system. Each agent in the system has a MAXQ decomposition of the MDP and aims to learn how to act for each task, which order to do the tasks, and how to coordinate with other agents. Coordination of agents is only done at the highest level of the decomposition. This was the critique by Marthi, because there are situations where agents must interact at a low level. An example would be coordinated navigation to avoid collisions. 


\section{Summary}
\label{Summary}
Marthi touches on a myriad of topics as he discusses the research conducted in his thesis. Starting with background on ``Flat" reinforcement learning, Marthi explains the foundations of the Semi-Markov Decision Process (SMDP), and partial programs upon which the research is built.
%and multieffector MDPs 

The first major contribution of the dissertation, the Concurrent ALisp language, is then explained. A description of the implementation follows. How learning algorithms are applied in Concurrent ALips then follows. These learning algorithms use the early foundations to describe how the learning using Concurrent ALisp is executed.

The experimental results then backup the principles of learning using Concurrent ALisp. The primary themes of partial programs, reward decomposition, and scalability are all tested.

\subsection{Background}
\label{Background}

\subsubsection{MDPs and SMDPs}
The research done by Marthi, along with much for the reinforcement learning field, builds upon Markov decision processes or MDPs \cite{Puterman:1994} \cite{bertsekas:1996}. MDPs are often used to model sequential decision making processes. An MDP can be defined as a tuple $M = (S, A, P, R, s_0)$. Each value of the tuple is defined as follows. 
\begin{itemize}
	\item $S$ - state space
	\item $A$ - action space
	\item $P$ - transition distribution
	\item $R$ - reward function. A function that maps a state, action, and next state $R(s,a,s')$ to a member in $\mathbb{R} \cup -\infty$
	\item $s_0$ - initial start state
\end{itemize}

For an MDP to be an accurate representation of the problem, two general properties have to be met or assumed. First, the current state must be derivable just from the last perception of the environment but the agent. Second, the Markov property is assumed. The Markov property states that the probability of entering a given state next only relies on the current state and the action taken from that state. No prior history before that state is taken into account. 

Markov decision processes can be solved or estimated with a multitude of different algorithms and approaches. The solution to an MDP is known as a \textit{policy}, denoted by $\pi$. A policy describes what actions an agent should take when in a given state. Two types of policies to focus on are stationary and non-stationary policies. 

A stationary policy is one in which it depends only on the last state, $\pi(s)$. A non-stationary policy is one in which the action decision relies on additional information than just the current state. Marthi focuses on non-stationary policies as he notes that most hierarchical reinforcement learning breaks agent behavior into tasks. As a result, the goal of an agent might not be recoverable from just the environment state. 

From MDPs, a modified version called a semi-Markov decision process (SMDP) can be described. An SMDP is an MDP that also includes a duration distribution for each state action pair. The reasoning behind adding a duration is that actions can take some amount of time to complete. From a hierarchical standpoint with tasks, one might imagine that a task takes a certain amount of time. The SMDP is build to incorporate that duration into the model. 

\subsubsection{Partial Programs}
\label{Partial Programs}
A goal for Marthi's research is to allow programmers to easily incorporate background knowledge into learning algorithms. To do this Marthi introduces ALisp \cite{Andre:2002} and partial programming. Partial programs aim to help programmers incorporate background knowledge while writing a program to learn based on the partial program. 

A partial program can thought of as constraints on which policies can be searched for while learning. A policy that finds an optimal policy given these constraints can be considered hierarchically optimal. ALisp is a language in which partial programs can be written. In ALisp, the foundations for Concurrent ALisp, the construct of a choice statements is added to allow for non-determinism in the program execution. This nondeterminism is used for selecting which set of statements to run next and what actions to take in the environment. The program then corresponds to a set of policies that can run the program and choose what to do at the choice statements. The choices made at each statement use a \textit{completion} of the program. 

Partial programs when combined with an underlying MDP create an SMDP, called the induced SMDP. The states in the SMDP are derived from the state of the MDP and the \textit{machine state}. The machine state can be thought of as program specific information about the partial program during execution. Actions in the SMDP are the choices made at the choice statements. The actions taken in the program then might result in some action being taken in the underlying environment. The reward for the SMDP is the reward gained during all actions between two choice statements. 

By creating an SMDP from the partial program, algorithms for SMDPs can be applied to the induced SMDP. Marthi concludes that finding the hierarchically optimal policy corresponding to the partial program is equivalent to finding the optimal stationary policy for the induced SMDP. This means that by looking at the SMDP, a hierarchically optimal execution of the partial program can be found. Since the partial program corresponds to constraints on policies for the MDP, finding a hierarchically optimal execution for the partial program means that a hierarchically optimal policy has been found for the MDP. 

Partial programs also allow for decomposition of the Q-function for learning algorithms. The Q-function can be thought of as the "action-value function". The Q-function is the expected reward if we start at $\textbf{s}$, a history of states, and do action $a$, then follow $\pi$. For partial programs, the Q-function can be decomposed into three components, $Q_r$, $Q_c$, $Q_e$. These are the expected reward received while doing the current choice, after the current choice until the subroutine ends, and after the current subroutine respectively.  

The motivation behind decomposing the Q-function is to simplify the learning process. Each part of the decomposition may only rely on a particular subset of variables that are part of the state. In these cases, state abstraction can be leveraged, and thus reduce the sample complexity of learning, since each value is learned independently. 


\subsection{Concurrent ALisp (CALisp)}
\label{CALisp}
In the case of multiple agents or a multieffector MDP, ALisp alone is no longer suitable for solving these problems. To attempt to make ALisp work with multiple agents, the partial program would then have to be written to account for multiple agents and require a much more complicated program with book keeping. This is due to the fact that the subroutine nature of ALisp is lost because each agent might be doing something different at every time step. Separate ALisp programs could be run instead to maintain the subroutine nature but in doing so it assumes there are no coordinated actions at a low-level. 

Concurrent ALisp is designed to solve these problems. This is Marthi's first major contribution. To sidestep the flaws of a large ALisp program or separate ALisp programs, Concurrent ALisp takes a multithreading approach to solving the problem.

The multithreading approach allows for the partial program to spawn new threads and specify a specific subroutine that is then executed in that thread. When the subroutine finishes, the thread dies. These threads will run independently from one another until a choice or action statement is reached. 

When a thread reaches a choice statement, the thread pauses execution and waits for other threads to arrive at a choice or action statement. All threads that are at a choice statement then make a joint choice. This choice is based on the \textit{completion} that is provided to the partial program. 

Threads also wait for each other at action statements. Once all threads are at an action statement, a joint action is taken. If some threads are at choice statements and others are at action statements, threads at the action statements wait, while threads at choice statements choose and continue until they are at an action statement.  

Unlike separate programs, these joint choices allow for coordination between subroutines. The CALisp partial programs also retains is subroutine structure, while taking care of all the bookkeeping for threading. This allows for slight modifications to the program to be made in order to adapt to multiple agents, rather than a complex rewrite. A multithreaded structure also exposes a threadwise parallel structure in addition to temporal structure which is later taken advantage of in the learning algorithms. 

Marthi goes on to define the formal structure of CALisp in his dissertation. The reader is referred to the thesis for exact details on the formalization of CALisp. The formalization describes how one might implement CALisp. 
%However, there are three important outcomes from the formalization: the classification of a state, how joint choice states operate, and how joint action states operate.

%A combined state $\omega$, can be one of three classes of state: a join choice state, a joint action state, or an internal state. A joint action state is a state where each of its threads with effectors (agents) has the status \textit{action}. Choice states are states that are not action states and no thread has the status of \textit{running}. Any state that is not an action or choice state is an \textit{internal state}. \textit{Action} and \textit{running} are statuses applied to threads. \textit{Action} means that a thread is at an action statement in the partial program, while \textit{running} means that the thread is currently running commands in the partial program but is not at a choice or action statement. 

%In a joint choice states, each of the threads actual choice is set according to what the completion specifies. The thread is stepped, and the status is returned to running. In a join action state, each effector is assigned an action

\subsection{Implementation of CALisp}
\label{Implementation}
As his second major contribution, Marthi created an implementation of his CALisp language. The implementation of CALisp for this thesis was written in Lisp.  This section of the dissertation is similar to code documentation and language specifics. This includes things like specifying types, function calls, parameters for functions, objects, and required methods for these objects. As a result, readers are referred to the thesis for details of the exact implementation rather than including a summarization of the language particulars in this critique. 

At a high level, the important part about the language implementation is the idea of extensibility. CALisp was implemented in an object-orient paradigm. Based on this paradigm, the implementation focused on defining objects that were necessary for any partial program to run, and objects that could be extended for additional functionality. Key examples of objects that were designed to be extensible are the Q-function, approximation architectures, and learning algorithm objects. These objects are described with a base implementation but also layout the functions needed and the behavior expected if one were to extend the object. 

Extensibility is important when implementing a language. Rarely is a user going to want only the pre-built functions of a language. This would mean that a user of the language would have little control over the behavior of objects native to the language and has little recourse in building their own objects to try and modify those behaviors. By allowing for users to extend objects, the user is given much more control over how a partial program in the language might behave. 

An example of extending a CALisp object is the ability to create a new approximation architecture. The language implementation comes with a default linear approximator. If a user wanted to instead use a neural network, the user could extend the approximation architecture object and implement the required methods. This is a very important feature for CALisp in order to allow CALisp to be used by others and to extend the life of CALisp. 

\subsection{Learning Algorithms in CALisp}
\label{Learning}
Several algorithms have been developed to learn an optimal policy in a flat MDP. Two popular algorithms that are also leveraged by Marthi are Q-learning \cite{Watkins:1992} and SARSA \cite{Sutton:1998}. Both build on the idea of dynamic programming and temporal-difference methods to find a solution to the MDP. Temporal difference methods work by storing an estimation of the value function for the policy, $\hat{V}^{\pi}$. Then after an action is taken according to the policy, an update of the estimated value function is applied according to equation \ref{TD-learning}

\begin{equation}
\label{TD-learning}
\hat{V}^{\pi}(s) \leftarrow \hat{V}^{\pi}(s) + \eta(r + \hat{V}^{\pi}(s') - \hat{V}^{\pi}(s))
\end{equation}

where $\eta$ is a learning rate and $r$ is the reward observed. 

Q-learning uses a temporal difference method but to estimate the optimal Q-function given an observation of a state, action, reward, and new state. The update performed is 

\begin{equation}
\label{q-learning}
\hat{Q}(s,a) \leftarrow \hat{Q}(s,a) + \eta(r + \max_{a'}\hat{Q}(s',a') - \hat{Q}(s,a))
\end{equation}
where $\eta$ is again a learning rate. A discount factor can be added to the equation to model the idea of future rewards being less valuable then current a reward. 

SARSA is similar to Q-learning, but rather than updating the Q-function estimation based on the maximum value of $(s', a')$, the update is made according the estimated Q-function determined by the current policy, $\pi(s',a')$. This makes SARSA an on-policy algorithm while Q-learning is an off-policy algorithm. The SARSA update is described as 

\begin{equation}
\label{SARSA}
\hat{Q}(s,a) \leftarrow \hat{Q}(s,a) + \eta(r + \hat{Q}(s', \pi(s',a')) - \hat{Q}(s,a))
\end{equation}
where $a'$ is chosen according to the policy. 

These algorithms, while designed for MDPs can be modified for SMDPs by adding a discount term that scales according to the duration of the task. For Q-learning an updated equation \ref{q-learning} for SMDPs would be 

\begin{equation}
\label{q-SMDP}
\hat{Q}(s,a) \leftarrow \hat{Q}(s,a) + \eta(r + \gamma^d \max_{a'}\hat{Q}(s',a') - \hat{Q}(s,a))
\end{equation}
where $d$ is the duration of the task. 

The modifications of these algorithms for SMDPs are important because, as discussed earlier, the partial programs written in CALisp with an underlying MDP can produce an induced SMDP. Thus, a simple way to write a learning algorithm for CALisp programs is to use an existing algorithm on the induced SMDP from the partial program. 

The SMDP Q-learning algorithm could thus be used in CALisp to learn the optimal stationary policy on the SMDP. As a reminder, the optimal stationary policy of the SMDP is equivalent to the hierarchically optimal completion of the partial program. This then corresponds to a hierarchically optimal solution for the MDP. As a result, a hierarchically optimal solution can be found to the MDP by finding the optimal stationary policy of the induced SMDP. 

\subsubsection{Decomposed Learning}
While applying a standard SMDP algorithm to CALisp works, Marthi points out that there is a better way and develops more optimal methods as his third major contribution.

For the SMDP Q-learning, rewards are not decomposed in any way which means that should there be a positive and negative reward from different subroutines, all subroutines are penalized according to the combined rewards. In addition, the decomposition of the Q-function that is available as part of a partial program in ALisp is lost in CALisp because subroutines now run in separate threads.

However, these problems are solved if a programmer can provide a way of decomposing the reward signal into per-thread rewards. This allows specific threads, and thus subroutines, to be associated with particular rewards. This then allows a threadwise decomposition of the Q-function across subroutine boundaries, like in ALisp. 

In order to make a threadwise decomposition, the variability in the number of threads has to be taken into account. Since threads are spawned only from other threads, this can be boiled down to taking into account a threads future rewards as well as rewards form uncreated descendants of the that thread. This is done by calculating the Q-function for a given thread by taking into account the relevant descendants of a thread. The reward received from the relevant descendants of threat $t$ for a set of combined states (called a trajectory) $\vec{\omega}$ can be defined as a random variable $\chi ^t(\omega)$. The threadwise Q-function can then be defined as 

\begin{equation}
Q^{\psi}_t (\omega, u) \triangleq E[\chi^t(\vec{\omega})]
\end{equation}  
for a state $\omega$, and a choice $u$ based on a completion $\psi$. This means that the Q-component for $t$ is the expected total reward from $t$ and all descendants of $t$. 

This then allows the combined Q-function for a CALisp program is proven by Marthi to be the sum of the threadwise Q-components in the current state. This is described as 

\begin{equation}
Q^{\psi} (\omega, u) = \sum_{t \in T(\omega)} Q^{\psi}_t (\omega, u) 
\end{equation}  

As a result, Marthi claims, an algorithm that learns the $Q_t$ corresponding to an optimal completion for all threads that could be seen during execution, the optimal completion can be recovered from the learned $Q_t$'s. This means that learning algorithms can target learning more specific Q-functions for subroutines possibly reduce the complexity of learning without losing the ability to learn the optimal completion. 

A temporal decomposition can then be defined per thread decomposition, similar to the ALisp partial program decomposition of the Q-function discussed in \ref{Partial Programs}. The biggest difference is that the temporal decomposition of the Q-function relies heavily on the implementation of CALisp since use of the machine state is used to determine which parts of the reward should be incorporated into which of the temporal Q-components. 

The decomposition results in temporal Q-components per thread of $Q_{t,r}$, $Q_{t,c}$, $Q_{t,e}$. $Q_{t,r}$ is the reward gained doing a particular action, or rewards all rewards gained until a choice block. $Q_{t,c}$ is subsequent rewards in the current choice block. $Q_{t,e}$ is the rewards outside the choice block.

These temporal Q-Components are then shown to equal the total threadwise Q-component by 

\begin{equation}
Q^{\psi}_t (\omega, u) =  Q^{\psi}_{t,r} (\omega, u) + Q^{\psi}_{t,c} (\omega, u) + Q^{\psi}_{t,e} (\omega, u)
\end{equation}  
This then allows a learning algorithm to focus even more narrowly on learning just the temporal decomposition of the threadwise Q-function, while still corresponding to an optimal completion of the partial program. 


\subsubsection{SARSA Implementation}
Marthi provides and implements two SARSA variants, one with threadwise decomposition and the other with temporal decomposition. Threadwise decomposition of SARSA is proved to converge to the optimal global Q-function, based on work done by Russell and Zimdars \cite{Russell:2003}, and Singh et al. \cite{Singh}. Russell showed threadwise updates are equivalent to standard SARSA updates. Singh wrote the SARSA convergence theorem. 

A conjecture is made that the temporally decomposed SARSA will converge the a optimal global Q-function. However no proof is provided. Even for ALisp, though, the temporal decomposition of SARSA has not been proved to converge. 


\subsection{Experimental Results}
\label{Experiments}
Three sets of experiments were run using CALisp, each with a different focus. The first focuses on the effect of incorporating prior knowledge through a partial program on learning. The second focuses on the performance between threadwise decomposition, temporal decomposition, and undecomposed algorithms. The third measures the scalability of algorithms. 

\subsubsection{Prior Knowledge experiment}
To test the role of prior knowledge in the form of a partial program, five different partial programs are compared, labeled 1-5. Each algorithm adds slightly more prior knowledge than the last and the fifth uses incorrect prior knowledge. 
\begin{enumerate}
\item equivalent to flat reinforcement learning.
\item specifies task hierarchies with no order.
\item adds some task ordering information
\item includes ordering information along with coordination information.
\item includes ``faulty prior knowledge"
\end{enumerate}

The environment for the test is a 3x2 grid world, where two peasants try to gather two of each type of resource. The resources are gold and wood. Despite the small world, it has over 4500 states and 49 actions per state. 

The learning algorithm used is a tabular representation of Q-Learning. The tabular representation was used to avoid any possible benefit to learning outside of the partial program, such as a good function approximator or reward shaping. This is the reason why a small example is chosen for the tabular representation. Even though it is a small example, the Q-function tables still has over 2 x $10^5$ entries. 

The results of the experiment show that with increasing prior knowledge in the partial program, the rate of convergence to an optimal policy also increased. Partial program number 4 converged the quickest, while partial program number 1 took the longest. Partial program 5 learned quickly, but ultimately converged to a suboptimal policy because of the incorrect prior knowledge.

\subsubsection{Decomposed learning experiments}
Three learning algorithms are tested to explore the role of decomposed learning. The algorithms compared are an undecomposed, threadwise decomposed, and temporally decomposed Q-learning algorithms. For each algorithm, linear approximation and reward shaping were used to make learning more tractable. Boltzman exploration was used for the exploration policy for all algorithms. 

Again, the environment is a resource gathering problem. In this case, the world is a 20x20 grid with 6 peasants. The goal was to gather 10 gold and 15 wood.

The results demonstrate that temporal decomposition converged to the optimal policy the fastest, very closely followed by threadwise decomposition. The difference between the two is difficult to see in the resulting graph provided, but the authors analysis seems believable. An explanation for the similar performance is based on the task tested having a relatively shallow task hierarchy. It is suggested that temporal decomposition might perform noticeably better than threadwise decomposition in these deeper hierarchy. However, no experimental results are provided on this front. 

\subsubsection{Scalability experiment}
Two experiments are conducted to test scalability: varying the number of peasants to collect resources, and varying the size of the world. Again, the goal is for peasants to collect resources in the environment. The algorithm used in the experiment is the temporally decomposed learning algorithm from the previous experiment.

Varying the number of peasants in a 15x15 grid world showed two stages of performance. For a small number of peasant learning took fewer steps but grew fairly quickly as the number of peasants increased to about 10 peasants. After 10 peasants the number of steps it takes to learn remains relatively similar for the rest of the number of peasants up to 40, which was the max tested. The rate of growth in learning time (in steps), after 10, looks roughly linear with the number of peasants. 

The reasoning given for the two different rates or learning is due to the task. Collisions provide a large negative reward, so for few peasants the learner might just need to learn to avoid collisions. As the number of peasants increase though, the learner might have to learn not to send multiple peasants to the same resource. 

In varying the map size, grids with side length of about 2, 5, 10, 20, 40, 60, and 100 were used with 5 peasants. The results show that as the map grows the number of steps needed until optimality increases in a roughly learn fashion with the size of the grid side length.

Reasoning as to this linear growth is based in the time it takes to get updates at the higher level of choices. Each choice probably takes longer because the grid is getting larger. This then creates fewer high-level updates to the Q-function, thus needing more steps to learn an optimal policy.

\section{Contributions}
\label{Contribs}
In this thesis, Marthi claims to provide three major contributions in his thesis: The specification of CALisp, an implementation of CALisp, and developing particular learning methods for CALisp that perform well. In addition, Marthi had three major goals in his thesis. These were to build a system that deals with multieffector problems easily, to create a way to incorporate prior knowledge into learning, and to show that in adding prior knowledge the time it takes to learn can be decreased. Marthi did a good job backing up his claims in this thesis. 

The specification for CALisp along with the foundations of why it works were well explained. Concurrent ALisp is backed by ALisp which grants it the niceties that were shown to come from ALisp. These included the mapping of ALisp to an induced SMDP. This mapping allows the same argument to be applied to CALisp, but it takes into account multieffectors and develops the benefits of threadwise architecture. 

The CALisp implementation is simultaneously a great contribution from Marthi, and not a very big one. The CALisp implementation is an important contribution to making sure the rest of the thesis holds because it is a concrete reference of CALisp that works and drives the experiments that are done. The implementation is done also done in an intelligent manner. The focus on the object oriented paradigm for CALisp allows users to build their own constructs in the language. 

The ability to allow users to extend objects to allow for behavior changes in their objects is what a language needs in order to be useful to any one other than the developer of the language. However, the implementation seems to not have garnered much interest in the field. The use of the implementation has not been seen outside of the same research group from which ALisp and CALisp was developed since publication. The concepts, however, have been used even as recently as 2017 in fields like Robo soccer \cite{CHRLKeepaway:2017}

Particular learning methods for CALisp were perhaps Marthi's largest contribution. Marthi showed that a standard Q-learning algorithm, applied to the induced SMDP of the CALisp partial program, could learn a hierarchically optimal solution to a mutlieffector problem with little to no extra work by the programmer. Better than that though was the threadwise decomposition of the the learning problem gained by using the CALisp structure. The threadwise decomposition was then experimentally shown to converge to an optimal policy faster than undecomposed learning. 

While the actual implementation might not be used any more, the formalization of how the threadwise decomposition worked lays the foundation for threadwise decomposition in general. While the development of the threadwise decomposition was with respect to CALisp, the concept is much more general. Marthi showed that for CALisp specifically, threadwise decomposition is both possible and has large positive impacts on learning time, shown experimentally.  

Temporal decomposition was another contribution of learning algorithms by Marthi. This decomposition has many of the same benefits as threadwise decomposition. It too lays a foundation for future work to look at possible temporal decomposition of Q-functions. The actual implementation of temporal decomposition showed that it was possible to do so and get equally or better results than threadwise decomposition. This contribution suffers from the derivation of the decomposition being tied closely to CALisp though. As a result, the decomposition is slightly less significant because it does not generalize as well.

Through these contributions Marthi managed to meet each of his goals in the thesis. CALisp is a well defined language built from the ground up for easily dealing with multieffector MDPs. Partial programs written in CALisp where shown to allow a programmable way of incorporating prior knowledge into learning. The partial programs provide an, albeit subjectively, easier method for using prior knowledge in conjunction with the learning algorithm than prior methods. Finally, Marthi was able to show experimentally that by adding prior knowledge in a partial program the amount of time needed to learn can be decreased. 

\section{Relevant Algorithms}
\label{relevant Algors}

\section{Applications of Concurrent Hierarchical RL}
\label{Applications}

Concurrent hierarchical reinforcement learning (CHRL) has many applicable areas to which the research from Marthi's thesis could be applied. Marthi decided to apply CHRL and CALisp specifically to the real-time strategy domain through Stratagus. Real-time strategy games are designed around the principle of having a large overarching strategy and specific low level strategies like resource collection. In general, real-time strategy games break strategy down into levels strategy known as \textit{micro} and \textit{macro} strategy. Often times, multiple micro-strategies are being carried out simultaneously. This distinct separation of strategy levels and concurrency is a perfect application for CHRL and CALisp. 

Throughout the thesis, the problem of resource gathering was explored. This is a fairly simplistic task where units collect resources to achieve some goal. Though mentioned in passing by the author, this is a very simply, and not very hierarchical example of CHRL. A better application of CHRL would be to apply CALisp to the more general problem of Stratagus, where there are multiple different units, each unit has a different goal, and each goal needs to be carried out in parallel. For example, a peasant needs to gather resources so that warriors can be built so that the enemy can be defeated. Goals would have to be set at different levels in the partial program, and concurrency would have to be used to execute all of the tasks needed. 

%alpha go if time

A more concrete application of CHRL through CALisp would be in robotics. Marthi mentions a case in control theory of using hierarchical control architectures \cite{Todorov:2005}. This could be a step towards full robotic motion. In the case mentioned, only the hierarchy of control is focused on. Allowing for CHRL though, would enable the robot to essentially multi-task, but keep a single goal as the target. An example might be a robot that aims to balance two objects in two different arms. The overall goal is to not let anything drop. This would then require the arms to work concurrently to execute the individual task of balancing. 

Command and control of multiple robots is a natural extension. Given a set of robots, a CALisp program could execute a goal of cleaning a room using updates from each independent cleaning robot in the environment. The goal for each robot might be different, but coordination between the robots would be needed to optimally clean.  



The use of partial programs in aiding prior knowledge incorporation would also be hugely beneficial in all of these cases. Currently prior knowledge, as described by Marthi, is typically infused into problems via reward shaping, or state approximation. Task decomposition is done by other algorithms as well such as MAXQ, but the syntax for doing so tend to be much less programmer friendly, which was a goal for Marthi. Partial programs also allow the code for the actual execution of a task to either be written right in CALisp, or communicate with other programs or systems in an intuitive manner. 

\section{Conclusion}
\label{conclusion}
Marthi's thesis, ``Concurrent Hierarchical Reinforcement Learning" contributes three major developments to the field. The specification of the CALisp programming language provides a construct in which to create partial programs that can be used for incorporating prior knowledge into the learning process for a mutlieffector MDP in an easy manner. An implementation of CALisp is provided as a reference implementation of CALisp and to experimentally demonstrate the power of CALisp. Multiple learning algorithms are derived and implemented in CALisp to showcase the benefits of CALisp. 

Experimentally, Marthi showed that prior knowledge infused into the partial program provides benefits in the rate of convergence to an optimal policy for learning algorithms. In addition, the learning algorithms derived based the structure of CALisp were shown to provide additional benefits in convergence rates. 

Between the experimental results and thorough development of the key contributions in the thesis, Marthi provides a compelling argument for the use of concurrency in hierarchical reinforcement learning, and for the use of a construct such as CALisp. 


\vskip 0.2in
\bibliography{CALispCritique}
\bibliographystyle{theapa}

\end{document}






