\documentclass[jair,twoside,11pt,theapa]{article}
\usepackage{jair, theapa, rawfonts, amssymb}

%\jairheading{1}{2018}{}{}{}
%\ShortHeadings{A Summary of Pairwise Saturations in ILP}
%{Robinson}
\firstpageno{1}

\begin{document}

\title{A Critique of the dissertation ``Concurrent Hierarchical Reinforcement Learning"}


\author{\name Max Robinson \email max.robinson@jhu.edu \\
       \addr Johns Hopkins University,\\
       Baltimore, MD 21218 USA
   }

% For research notes, remove the comment character in the line below.
%\researchnote APril3 

\maketitle

%\begin{abstract}
%In Drole \cite{Drole2017}
%\end{abstract}

\section{Introduction}
\label{Introduction}
\cite{Marthi:2006}

\section{Related Work}
\label{Related Work}

\section{Summary}
\label{Summary}
Marthi touches on a myriad of topics as he discusses the research conducted in his thesis. Starting with background on ``Flat" reinforcement learning, Marthi explains the foundations of the Semi-Markov Decision Process (SMDP), and partial programs upon which the research is built.
%and multieffector MDPs 

The first major contribution of the dissertation, the Concurrent ALisp language, is then explained. A description of the implementation follows. How learning algorithms are applied in Concurrent ALips then follows. These learning algorithms use the early foundations to describe how the learning using Concurrent ALisp is executed.

The experimental results then backup the principles of learning using Concurrent ALisp. The primary themes of partial programs, reward decomposition, and scalability are all tested.

\subsection{Background}
\label{Background}

\subsubsection{MDPs and SMDPs}
The research done by Marthi, along with much for the reinforcement learning field, builds upon Markov decision processes or MDPs \cite{Puterman:1994} \cite{bertsekas:1996}. MDPs are often used to model sequential decision making processes. An MDP can be defined as a tuple $M = (S, A, P, R, s_0)$. Each value of the tuple is defined as follows. 
\begin{itemize}
	\item $S$ - state space
	\item $A$ - action space
	\item $P$ - transition distribution
	\item $R$ - reward function. A function that maps a state, action, and next state $R(s,a,s')$ to a member in $\mathbb{R} \cup -\infty$
	\item $s_0$ - initial start state
\end{itemize}

For an MDP to be an accurate representation of the problem, two general properties have to be met or assumed. First, the current state must be derivable just from the last perception of the environment but the agent. Second, the Markov property is assumed. The Markov property states that the probability of entering a given state next only relies on the current state and the action taken from that state. No prior history before that state is taken into account. 

Markov decision processes can be solved or estimated with a multitude of different algorithms and approaches. The solution to an MDP is known as a \textit{policy}, denoted by $\pi$. A policy describes what actions an agent should take when in a given state. Two types of policies to focus on are stationary and non-stationary policies. 

A stationary policy is one in which it depends only on the last state, $\pi(s)$. A non-stationary policy is one in which the action decision relies on additional information than just the current state. Marthi focuses on non-stationary policies as he notes that most hierarchical reinforcement learning breaks agent behavior into tasks. As a result, the goal of an agent might not be recoverable from just the environment state. 

From MDPs, a modified version called a semi-Markov decision process (SMDP) can be described. An SMDP is an MDP that also includes a duration distribution for each state action pair. The reasoning behind adding a duration is that actions can take some amount of time to complete. From a hierarchical standpoint with tasks, one might imagine that a task takes a certain amount of time. The SMDP is build to incorporate that duration into the model. 

\subsubsection{Partial Programs}
A goal for Marthi's research is to allow programmers to easily incorporate background knowledge into learning algorithms. To do this Marthi introduces ALisp \cite{Andre:2002} and partial programming. Partial programs aim to help programmers incorporate background knowledge while writing a program to learn based on the partial program. 

A partial program can thought of as constraints on which policies can be searched for while learning. A policy that finds an optimal policy given these constraints can be considered hierarchically optimal. ALisp is a language in which partial programs can be written. In ALisp, the foundations for Concurrent ALisp, the construct of a choice statements is added to allow for non-determinism in the program execution. This nondeterminism is used for selecting which set of statements to run next and what actions to take in the environment. The program then corresponds to a set of policies that can run the program and choose what to do at the choice statements. The choices made at each statement use a \textit{completion} of the program. 

Partial programs when combined with an underlying MDP create an SMDP, called the induced SMDP. The states in the SMDP are derived from the state of the MDP and the \textit{machine state}. The machine state can be thought of as program specific information about the partial program during execution. Actions in the SMDP are the choices made at the choice statements. The actions taken in the program then might result in some action being taken in the underlying environment. The reward for the SMDP is the reward gained during all actions between two choice statements. 

By creating an SMDP from the partial program, algorithms for SMDPs can be applied to the induced SMDP. Marthi concludes that finding the hierarchically optimal policy corresponding to the partial program is equivalent to finding the optimal stationary policy for the induced SMDP. This means that by looking at the SMDP, a hierarchically optimal execution of the partial policy can be found.  

Partial programs also allow for decomposition of the Q-function for learning algorithms. The Q-function can be thought of as the "action-value function". The Q-function is the expected reward if we start at $\textbf{s}$, a history of states, and do action $a$, then follow $\pi$. For partial programs, the Q-function can be decomposed into three components, $Q_r$, $Q_c$, $Q_e$. These are the expected reward received while doing the current choice, after the current choice until the subroutine ends, and after the current subroutine respectively.  

The motivation behind decomposing the Q-function is to simplify the learning process. Each part of the decomposition may only rely on a particular subset of variables that are part of the state. In these cases, state abstraction can be leveraged, and thus reduce the sample complexity of learning, since each value is learned independently. 


\subsection{Concurrent ALisp (CALisp)}
\label{CALisp}
In the case of multiple agents or a multieffector MDP, ALisp alone is no longer suitable for solving these problems. To attempt to make ALisp work with multiple agents, the partial program would then have to be written to account for multiple agents and require a much more complicated program with book keeping. This is due to the fact that the subroutine nature of ALisp is lost because each agent might be doing something different at every time step. Separate ALisp programs could be run instead to maintain the subroutine nature but in doing so it assumes there are no coordinated actions at a low-level. 

Concurrent ALisp is designed to solve these problems. This is Marthi's first major contribution. To sidestep the flaws of a large ALisp program or separate ALisp programs, Concurrent ALisp takes a multithreading approach to solving the problem.

The multithreading approach allows for the partial program to spawn new threads and specify a specific subroutine that is then executed in that thread. When the subroutine finishes, the thread dies. These threads will run independently from one another until a choice or action statement is reached. 

When a thread reaches a choice statement, the thread pauses execution and waits for other threads to arrive at a choice or action statement. All threads that are at a choice statement then make a joint choice. This choice is based on the \textit{completion} that is provided to the partial program. 

Threads also wait for each other at action statements. Once all threads are at an action statement, a joint action is taken. If some threads are at choice statements and others are at action statements, threads at the action statements wait, while threads at choice statements choose and continue until they are at an action statement.  

Unlike separate programs, these joint choices allow for coordination between subroutines. The CALisp partial programs also retains is subroutine structure, while taking care of all the bookkeeping for threading. This allows for slight modifications to the program to be made in order to adapt to multiple agents, rather than a complex rewrite. A multithreaded structure also exposes a threadwise parallel structure in addition to temporal structure which is later taken advantage of in the learning algorithms. 

Marthi goes on to define the formal structure of CALisp in his dissertation. The reader is referred to the thesis for exact details on the formalization of CALisp. The formalization describes how one might implement CALisp. 
%However, there are three important outcomes from the formalization: the classification of a state, how joint choice states operate, and how joint action states operate.

%A combined state $\omega$, can be one of three classes of state: a join choice state, a joint action state, or an internal state. A joint action state is a state where each of its threads with effectors (agents) has the status \textit{action}. Choice states are states that are not action states and no thread has the status of \textit{running}. Any state that is not an action or choice state is an \textit{internal state}. \textit{Action} and \textit{running} are statuses applied to threads. \textit{Action} means that a thread is at an action statement in the partial program, while \textit{running} means that the thread is currently running commands in the partial program but is not at a choice or action statement. 

%In a joint choice states, each of the threads actual choice is set according to what the completion specifies. The thread is stepped, and the status is returned to running. In a join action state, each effector is assigned an action

\subsection{Implementation of CALisp}
\label{Implementation}
As his second major contribution, Marthi created an implementation of his CALisp language. The implementation of CALisp for this thesis was written in Lisp.  This section of the dissertation is similar to code documentation and language specifics. This includes things like specifying types, function calls, parameters for functions, objects, and required methods for these objects. As a result, readers are referred to the thesis for details of the exact implementation rather than including a summarization of the language particulars in this critique. 

At a high level, the important part about the language implementation was the idea of extensibility. CALisp was implemented in an object-orient paradigm. 

%talk about how object oriented style allows for others to implement Q-learning or other learning algorithms easily into the CALisp system. 


\subsection{Learning Algorithms}
\label{Learning}

\subsection{Experimental Results}
\label{Experiments}

\section{Contributions}
\label{Contribs}

\section{Relevant Algorithms}
\label{relevant Algors}

\section{Applications of Concurrent Hierarchical RL}
\label{Applications}

\section{Conclusion}
\label{conclusion}

\vskip 0.2in
\bibliography{CALispCritique}
\bibliographystyle{theapa}

\end{document}






