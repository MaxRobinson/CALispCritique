\documentclass[jair,twoside,11pt,theapa]{article}
\usepackage{jair, theapa, rawfonts, amssymb}

%\jairheading{1}{2018}{}{}{}
%\ShortHeadings{A Summary of Pairwise Saturations in ILP}
%{Robinson}
\firstpageno{1}

\begin{document}

\title{A Critique of the dissertation ``Concurrent Hierarchical Reinforcement Learning"}


\author{\name Max Robinson \email max.robinson@jhu.edu \\
       \addr Johns Hopkins University,\\
       Baltimore, MD 21218 USA
   }

% For research notes, remove the comment character in the line below.
%\researchnote APril3 

\maketitle

%\begin{abstract}
%In Drole \cite{Drole2017}
%\end{abstract}

\section{Introduction}
\label{Introduction}
\cite{Marthi:2006}

\section{Related Work}
\label{Related Work}

\section{Summary}
\label{Summary}
Marthi touches on a myriad of topics as he discusses the research conducted in his thesis. Starting with background on ``Flat" reinforcement learning, Marthi explains the foundations of the Semi-Markov Decision Process (SMDP), and partial programs upon which the research is built.
%and multieffector MDPs 

The first major contribution of the dissertation, the Concurrent ALisp language, is then explained. A description of the implementation follows. How learning algorithms are applied in Concurrent ALips then follows. These learning algorithms use the early foundations to describe how the learning using Concurrent ALisp is executed.

The experimental results then backup the principles of learning using Concurrent ALisp. The primary themes of partial programs, reward decomposition, and scalability are all tested.

\subsection{Background}
\label{Background}

\subsubsection{MDPs and SMDPs}
The research done by Marthi, along with much for the reinforcement learning field, builds upon Markov decision processes or MDPs \cite{Puterman:1994} \cite{bertsekas:1996}. MDPs are often used to model sequential decision making processes. An MDP can be defined as a tuple $M = (S, A, P, R, s_0)$. Each value of the tuple is defined as follows. 
\begin{itemize}
	\item $S$ - state space
	\item $A$ - action space
	\item $P$ - transition distribution
	\item $R$ - reward function. A function that maps a state, action, and next state $R(s,a,s')$ to a member in $\mathbb{R} \cup -\infty$
	\item $s_0$ - initial start state
\end{itemize}

For an MDP to be an accurate representation of the problem, two general properties have to be met or assumed. First, the current state must be derivable just from the last perception of the environment but the agent. Second, the Markov property is assumed. The Markov property states that the probability of entering a given state next only relies on the current state and the action taken from that state. No prior history before that state is taken into account. 

Markov decision processes can be solved or estimated with a multitude of different algorithms and approaches. The solution to an MDP is known as a \textit{policy}, denoted by $\pi$. A policy describes what actions an agent should take when in a given state. Two types of policies to focus on are stationary and non-stationary policies. 

A stationary policy is one in which it depends only on the last state, $\pi(s)$. A non-stationary policy is one in which the action decision relies on additional information than just the current state. Marthi focuses on non-stationary policies as he notes that most hierarchical reinforcement learning breaks agent behavior into tasks. As a result, the goal of an agent might not be recoverable from just the environment state. 

From MDPs, a modified version called a semi-Markov decision process (SMDP) can be described. An SMDP is an MDP that also includes a duration distribution for each state action pair. The reasoning behind adding a duration is that actions can take some amount of time to complete. From a hierarchical standpoint with tasks, one might imagine that a task takes a certain amount of time. The SMDP is build to incorporate that duration into the model. 

\subsubsection{Partial Programs}
\label{Partial Programs}
A goal for Marthi's research is to allow programmers to easily incorporate background knowledge into learning algorithms. To do this Marthi introduces ALisp \cite{Andre:2002} and partial programming. Partial programs aim to help programmers incorporate background knowledge while writing a program to learn based on the partial program. 

A partial program can thought of as constraints on which policies can be searched for while learning. A policy that finds an optimal policy given these constraints can be considered hierarchically optimal. ALisp is a language in which partial programs can be written. In ALisp, the foundations for Concurrent ALisp, the construct of a choice statements is added to allow for non-determinism in the program execution. This nondeterminism is used for selecting which set of statements to run next and what actions to take in the environment. The program then corresponds to a set of policies that can run the program and choose what to do at the choice statements. The choices made at each statement use a \textit{completion} of the program. 

Partial programs when combined with an underlying MDP create an SMDP, called the induced SMDP. The states in the SMDP are derived from the state of the MDP and the \textit{machine state}. The machine state can be thought of as program specific information about the partial program during execution. Actions in the SMDP are the choices made at the choice statements. The actions taken in the program then might result in some action being taken in the underlying environment. The reward for the SMDP is the reward gained during all actions between two choice statements. 

By creating an SMDP from the partial program, algorithms for SMDPs can be applied to the induced SMDP. Marthi concludes that finding the hierarchically optimal policy corresponding to the partial program is equivalent to finding the optimal stationary policy for the induced SMDP. This means that by looking at the SMDP, a hierarchically optimal execution of the partial program can be found. Since the partial program corresponds to constraints on policies for the MDP, finding a hierarchically optimal execution for the partial program means that a hierarchically optimal policy has been found for the MDP. 

Partial programs also allow for decomposition of the Q-function for learning algorithms. The Q-function can be thought of as the "action-value function". The Q-function is the expected reward if we start at $\textbf{s}$, a history of states, and do action $a$, then follow $\pi$. For partial programs, the Q-function can be decomposed into three components, $Q_r$, $Q_c$, $Q_e$. These are the expected reward received while doing the current choice, after the current choice until the subroutine ends, and after the current subroutine respectively.  

The motivation behind decomposing the Q-function is to simplify the learning process. Each part of the decomposition may only rely on a particular subset of variables that are part of the state. In these cases, state abstraction can be leveraged, and thus reduce the sample complexity of learning, since each value is learned independently. 


\subsection{Concurrent ALisp (CALisp)}
\label{CALisp}
In the case of multiple agents or a multieffector MDP, ALisp alone is no longer suitable for solving these problems. To attempt to make ALisp work with multiple agents, the partial program would then have to be written to account for multiple agents and require a much more complicated program with book keeping. This is due to the fact that the subroutine nature of ALisp is lost because each agent might be doing something different at every time step. Separate ALisp programs could be run instead to maintain the subroutine nature but in doing so it assumes there are no coordinated actions at a low-level. 

Concurrent ALisp is designed to solve these problems. This is Marthi's first major contribution. To sidestep the flaws of a large ALisp program or separate ALisp programs, Concurrent ALisp takes a multithreading approach to solving the problem.

The multithreading approach allows for the partial program to spawn new threads and specify a specific subroutine that is then executed in that thread. When the subroutine finishes, the thread dies. These threads will run independently from one another until a choice or action statement is reached. 

When a thread reaches a choice statement, the thread pauses execution and waits for other threads to arrive at a choice or action statement. All threads that are at a choice statement then make a joint choice. This choice is based on the \textit{completion} that is provided to the partial program. 

Threads also wait for each other at action statements. Once all threads are at an action statement, a joint action is taken. If some threads are at choice statements and others are at action statements, threads at the action statements wait, while threads at choice statements choose and continue until they are at an action statement.  

Unlike separate programs, these joint choices allow for coordination between subroutines. The CALisp partial programs also retains is subroutine structure, while taking care of all the bookkeeping for threading. This allows for slight modifications to the program to be made in order to adapt to multiple agents, rather than a complex rewrite. A multithreaded structure also exposes a threadwise parallel structure in addition to temporal structure which is later taken advantage of in the learning algorithms. 

Marthi goes on to define the formal structure of CALisp in his dissertation. The reader is referred to the thesis for exact details on the formalization of CALisp. The formalization describes how one might implement CALisp. 
%However, there are three important outcomes from the formalization: the classification of a state, how joint choice states operate, and how joint action states operate.

%A combined state $\omega$, can be one of three classes of state: a join choice state, a joint action state, or an internal state. A joint action state is a state where each of its threads with effectors (agents) has the status \textit{action}. Choice states are states that are not action states and no thread has the status of \textit{running}. Any state that is not an action or choice state is an \textit{internal state}. \textit{Action} and \textit{running} are statuses applied to threads. \textit{Action} means that a thread is at an action statement in the partial program, while \textit{running} means that the thread is currently running commands in the partial program but is not at a choice or action statement. 

%In a joint choice states, each of the threads actual choice is set according to what the completion specifies. The thread is stepped, and the status is returned to running. In a join action state, each effector is assigned an action

\subsection{Implementation of CALisp}
\label{Implementation}
As his second major contribution, Marthi created an implementation of his CALisp language. The implementation of CALisp for this thesis was written in Lisp.  This section of the dissertation is similar to code documentation and language specifics. This includes things like specifying types, function calls, parameters for functions, objects, and required methods for these objects. As a result, readers are referred to the thesis for details of the exact implementation rather than including a summarization of the language particulars in this critique. 

At a high level, the important part about the language implementation is the idea of extensibility. CALisp was implemented in an object-orient paradigm. Based on this paradigm, the implementation focused on defining objects that were necessary for any partial program to run, and objects that could be extended for additional functionality. Key examples of objects that were designed to be extensible are the Q-function, approximation architectures, and learning algorithm objects. These objects are described with a base implementation but also layout the functions needed and the behavior expected if one were to extend the object. 

Extensibility is important when implementing a language. Rarely is a user going to want only the pre-built functions of a language. This would mean that a user of the language would have little control over the behavior of objects native to the language and has little recourse in building their own objects to try and modify those behaviors. By allowing for users to extend objects, the user is given much more control over how a partial program in the language might behave. 

An example of extending a CALisp object is the ability to create a new approximation architecture. The language implementation comes with a default linear approximator. If a user wanted to instead use a neural network, the user could extend the approximation architecture object and implement the required methods. This is a very important feature for CALisp in order to allow CALisp to be used by others and to extend the life of CALisp. 

\subsection{Learning Algorithms in CALisp}
\label{Learning}
Several algorithms have been developed to learn an optimal policy in a flat MDP. Two popular algorithms that are also leveraged by Marthi are Q-learning \cite{Watkins:1992} and SARSA \cite{Sutton:1998}. Both build on the idea of dynamic programming and temporal-difference methods to find a solution to the MDP. Temporal difference methods work by storing an estimation of the value function for the policy, $\hat{V}^{\pi}$. Then after an action is taken according to the policy, an update of the estimated value function is applied according to equation \ref{TD-learning}

\begin{equation}
\label{TD-learning}
\hat{V}^{\pi}(s) \leftarrow \hat{V}^{\pi}(s) + \eta(r + \hat{V}^{\pi}(s') - \hat{V}^{\pi}(s))
\end{equation}

where $\eta$ is a learning rate and $r$ is the reward observed. 

Q-learning uses a temporal difference method but to estimate the optimal Q-function given an observation of a state, action, reward, and new state. The update performed is 

\begin{equation}
\label{q-learning}
\hat{Q}(s,a) \leftarrow \hat{Q}(s,a) + \eta(r + \max_{a'}\hat{Q}(s',a') - \hat{Q}(s,a))
\end{equation}
where $\eta$ is again a learning rate. A discount factor can be added to the equation to model the idea of future rewards being less valuable then current a reward. 

SARSA is similar to Q-learning, but rather than updating the Q-function estimation based on the maximum value of $(s', a')$, the update is made according the estimated Q-function determined by the current policy, $\pi(s',a')$. This makes SARSA an on-policy algorithm while Q-learning is an off-policy algorithm. The SARSA update is described as 

\begin{equation}
\label{SARSA}
\hat{Q}(s,a) \leftarrow \hat{Q}(s,a) + \eta(r + \hat{Q}(s', \pi(s',a')) - \hat{Q}(s,a))
\end{equation}
where $a'$ is chosen according to the policy. 

These algorithms, while designed for MDPs can be modified for SMDPs by adding a discount term that scales according to the duration of the task. For Q-learning an updated equation \ref{q-learning} for SMDPs would be 

\begin{equation}
\label{q-SMDP}
\hat{Q}(s,a) \leftarrow \hat{Q}(s,a) + \eta(r + \gamma^d \max_{a'}\hat{Q}(s',a') - \hat{Q}(s,a))
\end{equation}
where $d$ is the duration of the task. 

The modifications of these algorithms for SMDPs are important because, as discussed earlier, the partial programs written in CALisp with an underlying MDP can produce an induced SMDP. Thus, a simple way to write a learning algorithm for CALisp programs is to use an existing algorithm on the induced SMDP from the partial program. 

The SMDP Q-learning algorithm could thus be used in CALisp to learn the optimal stationary policy on the SMDP. As a reminder, the optimal stationary policy of the SMDP is equivalent to the hierarchically optimal completion of the partial program. This then corresponds to a hierarchically optimal solution for the MDP. As a result, a hierarchically optimal solution can be found to the MDP by finding the optimal stationary policy of the induced SMDP. 

\subsubsection{Decomposed Learning}
While applying a standard SMDP algorithm to CALisp works, Marthi points out that there is a better way. For the SMDP Q-learning, rewards are not decomposed in any way which means that should there be a positive and negative reward from different subroutines, all subroutines are penalized according to the combined rewards. In addition, the decomposition of the Q-function that is available as part of a partial program in ALisp is lost in CALisp because subroutines now run in separate threads.

However, these problems are solved if a programmer can provide a way of decomposing the reward signal into per-thread rewards. This allows specific threads, and thus subroutines, to be associated with particular rewards. This then allows a threadwise decomposition of the Q-function across subroutine boundaries, like in ALisp. 

In order to make a threadwise decomposition, the variability in the number of threads has to be taken into account. Since threads are spawned only from other threads, this can be boiled down to taking into account a threads future rewards as well as rewards form uncreated descendants of the that thread. This is done by calculating the Q-function for a given thread by taking into account the relevant descendants of a thread. The reward received from the relevant descendants of threat $t$ for a set of combined states (called a trajectory) $\vec{\omega}$ can be defined as a random variable $\chi ^t(\omega)$. The threadwise Q-function can then be defined as 

\begin{equation}
Q^{\psi}_t (\omega, u) \triangleq E[\chi^t(\vec{\omega})]
\end{equation}  
for a state $\omega$, and a choice $u$ based on a completion $\psi$. This means that the Q-component for $t$ is the expected total reward from $t$ and all descendants of $t$. 

This then allows the combined Q-function for a CALisp program is proven by Marthi to be the sum of the threadwise Q-components in the current state. This is described as 

\begin{equation}
Q^{\psi} (\omega, u) = \sum_{t \in T(\omega)} Q^{\psi}_t (\omega, u) 
\end{equation}  

As a result, Marthi claims, an algorithm that learns the $Q_t$ corresponding to an optimal completion for all threads that could be seen during execution, the optimal completion can be recovered from the learned $Q_t$'s. This means that learning algorithms can target learning more specific Q-functions for subroutines possibly reduce the complexity of learning without losing the ability to learn the optimal completion. 

A temporal decomposition can then be defined per thread decomposition, similar to the ALisp partial program decomposition of the Q-function discussed in \ref{Partial Programs}. The biggest difference is that the temporal decomposition of the Q-function relies heavily on the implementation of CALisp since use of the machine state is used to determine which parts of the reward should be incorporated into which of the temporal Q-components. 

The decomposition results in temporal Q-components per thread of $Q_{t,r}$, $Q_{t,c}$, $Q_{t,e}$. $Q_{t,r}$ is the reward gained doing a particular action, or rewards all rewards gained until a choice block. $Q_{t,c}$ is subsequent rewards in the current choice block. $Q_{t,e}$ is the rewards outside the choice block.

These temporal Q-Components are then shown to equal the total threadwise Q-component by 

\begin{equation}
Q^{\psi}_t (\omega, u) =  Q^{\psi}_{t,r} (\omega, u) + Q^{\psi}_{t,c} (\omega, u) + Q^{\psi}_{t,e} (\omega, u)
\end{equation}  
This then allows a learning algorithm to focus even more narrowly on learning just the temporal decomposition of the threadwise Q-function, while still corresponding to an optimal completion of the partial program. 


\subsubsection{SARSA Implementation}
Marthi provides and implements two SARSA variants, one with threadwise decomposition and the other with temporal decomposition. Threadwise decomposition of SARSA is proved to converge to the optimal global Q-function, based on work done by Russell and Zimdars \cite{Russell:2003}, and Singh et al. \cite{Singh}. Russell showed threadwise updates are equivalent to standard SARSA updates. Singh wrote the SARSA convergence theorem. 

A conjecture is made that the temporally decomposed SARSA will converge the a optimal global Q-function. However no proof is provided. Even for ALisp, though, the temporal decomposition of SARSA has not been proved to converge. 


\subsection{Experimental Results}
\label{Experiments}

\section{Contributions}
\label{Contribs}

\section{Relevant Algorithms}
\label{relevant Algors}

\section{Applications of Concurrent Hierarchical RL}
\label{Applications}

\section{Conclusion}
\label{conclusion}

\vskip 0.2in
\bibliography{CALispCritique}
\bibliographystyle{theapa}

\end{document}






